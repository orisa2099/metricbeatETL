input {
  beats {
    port => 5044
  }
}

filter {
  # Extract nested fields to top level for easier access
  mutate {
    add_field => {
      "host_name" => "%{[host][name]}"
      "event_module" => "%{[event][module]}"
      "metricset_name" => "%{[metricset][name]}"
      "event_dataset" => "%{[event][dataset]}"
    }
  }

  # Convert @timestamp to a PostgreSQL-compatible string format
  ruby {
    code => '
      require "time"
      ts = event.get("@timestamp")
      if ts.is_a?(LogStash::Timestamp)
        # Format: YYYY-MM-DD HH:MM:SS.mmm
        event.set("event_time", ts.time.strftime("%Y-%m-%d %H:%M:%S.%L"))
      end
    '
  }

  # Create raw_event JSON (must come AFTER extracting fields)
  ruby {
    code => '
      require "json"
      event.set("raw_event", event.to_hash.to_json)
    '
  }

  # Optional cleanup
  mutate {
    remove_field => [
      "@version",
      "ecs",
      "agent",
      "input"
    ]
  }
}

output {
  # Add stdout for debugging
  stdout {
    codec => rubydebug {
      metadata => true
    }
  }

  jdbc {
    driver_jar_path => "/usr/share/logstash/jdbc/postgresql-42.7.2.jar"
    driver_class => "org.postgresql.Driver"
    connection_string => "jdbc:postgresql://postgres:5432/metricsdb"
    username => "metricsuser"
    password => "metricspassword"

    # Use proper PostgreSQL timestamp casting and JSONB for raw_event
    statement => [
      "INSERT INTO metricbeat.metricbeat_events (event_time, host_name, event_module, metricset_name, event_dataset, raw_event) VALUES (?::timestamp, ?, ?, ?, ?, ?::jsonb)",
      "event_time",
      "host_name",
      "event_module",
      "metricset_name",
      "event_dataset",
      "raw_event"
    ]
  }
}